#!/usr/bin/env python3

import concurrent.futures
import getopt
import multiprocessing
import os
import sys

import netCDF4 as nc

import iwp.transforms

def print_usage( program_name, file_handle=sys.stdout ):
    """
    Prints the script's usage to standard output.

    Takes 2 arguments:

      program_name - Name of the program currently executing.
      file_handle  - File handle to print to.  If omitted, defaults to standard output.

    Returns nothing.

    """

    usage_str = \
"""{program_name:s} [-f] [-h] [-n <number_workers>] [-v] <reference_name> <netcdf_path>[,<netcdf_path>[...]] <output_name>:<transform_spec> [<output_name>:<transform_spec> [...]]

    Post-processes on-disk netCDF4 files by transforming a single reference variable,
    <reference_name>, into one or more output variables, <output_name>, according to
    the IWP Transforms module's transform specifications.  Processed data are written
    to the same file, <netcdf_path>, by creating new variables.  Overwriting existing
    variables is disabled by default to prevent accidental modification of pristine
    datasets.

    The mapping between reference variable and output variables is specified via one
    or more named transform specifications, <output_name>:<transform_spec>.  The
    contents of <output_name> are generated by applying <transform_spec> to the
    <reference_name> variable in each file.  <transform_spec> must be known to the
    IWP Transforms module and adhere to the following structure:

       <transform_name>:<parameters>

    where <parameters> are colon-delimited lists of (key, value) pairs separated by
    equals signs.  See iwp.transforms.lookup_transform() for a list of transforms
    and their parameters.

    For example, the following named transform demonstrates how to specify a
    symmetric Morlet 2D continuous wavelet transform, with a preferred angle of 50
    degrees, that takes the maximum modulus across multiple length scales and writes
    it to the 'morlet+-50' variable:

      morlet+-50:symmetric_morlet_max_modulus:alpha=50:scales=2,4,8,16,32

    Transformations may be applied in parallel using a process pool to accelerate
    batch processing.

      NOTE: Coarse-grained parallelism is achieved by concurrently processing
            multiple netCDF4 files, rather than transforming as single netCDF4 file
            in parallel.

    The command line options shown above are described below:

        -f                   Force overwriting existing variables encountered.  If
                             omitted, execution is prevented if any of the <netcdf_path>'s
                             supplied have a requested <output_name>.
        -h                   Print this help message and exit.
        -n <number_workers>  Specifies transforms should be applied in parallel using
                             <number_workers> processes.  Specifying as 0 results in
                             one process per core present on the local system.
        -v                   Enable verbose execution.  Status messages about progress
                             are written to standard output.  If omitted, defaults
                             to normal execution.

                               NOTE: Be careful enabling verbosity when parallel
                                     processing is enabled.  Status messages are not
                                     ordered in any way and will likely be a jumbled
                                     mess.
""".format(
    program_name=program_name
)

    print( usage_str, file=file_handle )

def parse_command_line( argv ):
    """
    Parses command line arguments for generating on-disk labeling data.

    See print_usage() for the structure of the options and arguments parsed.

    Raises ValueError if there is an issue parsing arguments.  This may occur when
    an incorrect number of arguments are supplied, if an invalid argument is supplied,
    or if an unknown option is parsed.

    Takes 1 argument:

      argv - List of strings representing the command line to parse.  Assumes the
             first string is the name of the script executing.

    Returns 2 values:

      options   - Object whose attributes represent the optional flags parsed.  Contains
                  at least the following:

                      .force_flag     - Flag specifying whether existing variables
                                        should be overwritten.
                      .number_workers - Non-negative integer specifying the number
                                        of workers to use during transformations.
                                        May be specified as 0 to launch as many
                                        workers as there are cores on the system.
                      .verbose_flag   - Flag specifying verbose execution.

                  NOTE: Will be None if execution is not required.

      arguments - Object whose attributes represent the positional arguments parsed.
                  Contains at least the following:

                      .netcdf_paths     - List of IWP netCDF4 paths to post-process.
                      .output_names     - List of variable names to write the
                                          transformation outputs to.  One per
                                          transform in .transform_specs.
                      .reference_name   - Name of the reference variable that will
                                          be transformed.
                      .transform_specs  - List of iwp.transforms transformation
                                          specifications.  One per variable in
                                          .output_names.

                  NOTE: Will be None if execution is not required.

    """

    # indices into argv's positional arguments specifying each of the required
    # arguments.
    ARG_REFERENCE_VARIABLE_NAME  = 0
    ARG_NETCDF4_PATHS            = 1
    ARG_TRANSFORM_SPECIFICATIONS = 2
    NUMBER_ARGUMENTS             = ARG_TRANSFORM_SPECIFICATIONS + 1

    # empty class designed to hold name values.
    #
    # XXX: this is a kludge until we sort out how to replace everything with
    #      argparse.
    #
    class _Arguments( object ):
        pass

    options, arguments = _Arguments(), _Arguments()

    # set defaults for each of the options.
    #
    # default to safe, serial execution that is quiet.
    options.force_flag     = False
    options.number_workers = 1
    options.verbose_flag   = False

    # parse our command line options.
    try:
        option_flags, positional_arguments = getopt.getopt( argv[1:], "fhn:v" )
    except getopt.GetoptError as error:
        raise ValueError( "Error processing option: {:s}\n".format( str( error ) ) )

    # handle any valid options that were presented.
    for option, option_value in option_flags:
        if option == "-f":
            options.force_flag = True
        elif option == "-h":
            print_usage( argv[0] )
            return (None, None)
        elif option == "-n":
            options.number_workers = option_value
        elif option == "-v":
            options.verbose_flag = True

    # ensure we have the correct number of arguments.
    if len( positional_arguments ) < NUMBER_ARGUMENTS:
        raise ValueError( "Incorrect number of arguments.  Expected at least "
                          "{:d}, received {:d}.".format(
            NUMBER_ARGUMENTS,
            len( positional_arguments ) ) )

    # map the positional arguments to named variables.
    arguments.reference_name  = positional_arguments[ARG_REFERENCE_VARIABLE_NAME]
    arguments.netcdf_paths    = positional_arguments[ARG_NETCDF4_PATHS].split( "," )
    arguments.output_names    = []
    arguments.transform_specs = []

    named_transform_specs = positional_arguments[ARG_TRANSFORM_SPECIFICATIONS:]

    # validate each of the named transform specifications.
    for transform_index, named_transform_spec in enumerate( named_transform_specs ):
        # separate the name from the specification.
        try:
            output_name, transform_spec = named_transform_spec.split( ":",
                                                                      maxsplit=1 )
        except:
            raise ValueError( "Named transform #{:d} is not of the form "
                              "<output_name>:<transform_spec> ('{:s}').".format(
                                  transform_index + 1,
                                  named_transform_spec ) )

        # make sure both the name and the specification aren't obviously
        # invalid.
        if len( output_name ) == 0:
            raise ValueError( "Named transform #{:d} has an empty output "
                              "variable name ('{:s}').".format(
                                  transform_index + 1,
                                  named_transform_spec ) )
        elif len( transform_spec ) == 0:
            raise ValueError( "Named transform #{:d} has an empty transform "
                              "specification ('{:s}').".format(
                                  transform_index + 1,
                                  named_transform_spec ) )

        # keep track of the name/transform specifications.  we'll parse/validate
        # the specifications later.
        arguments.output_names.append( output_name )
        arguments.transform_specs.append( transform_spec )

    # let the user know if they are wasting time computing a transform only to
    # overwrite it later in this run.
    if len( arguments.output_names ) != len( set ( arguments.output_names ) ):
        raise ValueError( "At least one output variable has been specified "
                          "more than once ({:s}).  Refusing to execute transforms "
                          "whose outputs are discarded.".format(
                              ", ".join( map( lambda name: "'" + name + "'",
                                              arguments.output_names ) ) ) )

    # also let the user know if they're wasting time applying the same
    # transforms more than once to a given file.
    #
    # NOTE: we do this both to avoid wasted work as well as to avoid concurrent
    #       writes to the underlying file when parallel execution was requested.
    #
    if len( arguments.netcdf_paths ) != len( set ( arguments.netcdf_paths ) ):
        raise ValueError( "At least one netCDF4 path has been specified "
                          "more than once ({:s}).  Refusing to execute transforms "
                          "whose outputs are discarded.".format(
                              ", ".join( map( lambda path: "'" + path + "'",
                                              arguments.netcdf_paths ) ) ) )

    # ensure that we got a sensible number of workers.
    try:
        options.number_workers = int( options.number_workers )

        if options.number_workers < 0:
            raise ValueError
    except:
        raise ValueError( "Invalid number of workers provided ({}).  Must be a "
                          "non-negative integer.".format(
                              options.number_workers ) )

    # get an explicit number of workers if the caller requested auto-detect.
    if options.number_workers == 0:
        options.number_workers = os.cpu_count()

    return options, arguments


def main( argv ):
    """
    Parses the supplied command line arguments and renders IWP data as on-disk
    images for labeling via an external tool.

    See print_usage() for details on execution and command line arguments.

    Takes 1 argument:

      argv - List of command line arguments and options, including the executing script.

    Returns 1 value:

      return_value - Integer status code.  Zero if execution was successful, non-zero
                     otherwise.

    """

    try:
        options, arguments = parse_command_line( argv )
    except Exception as e:
        print( str( e ), file=sys.stderr )
        return 1

    # return success in the case where normal execution is not required, but
    # we're not in an exceptional situation (e.g. requested help).
    if (options is None) and (arguments is None):
        return 0

    # validate the netCDF paths provided so we can identify problems up front
    # rather than once we've launched our parallel workers.
    for netcdf_path in arguments.netcdf_paths:

        # ensure the path exists.
        if not os.path.isfile( netcdf_path ):
            print( "netCDF4 path '{:s}' does not exist.".format(
                netcdf_path ),
                   file=sys.stderr )

            return 1

        # open the file and verify we have a reference variable.
        try:
            with nc.Dataset( netcdf_path, mode="r" ) as netcdf_file:

                if arguments.reference_name not in netcdf_file.variables:
                    raise RuntimeError( "The reference variable specified, "
                                        "'{:s}', is not in '{:s}'.  "
                                        "Cannot apply transforms to it.".format(
                                            arguments.reference_name,
                                            netcdf_path ) )

                # check to see if we're overwriting any files.
                if not options.force_flag:
                    for output_name in arguments.output_names:
                        if output_name in netcdf_file.variables:
                            raise RuntimeError( "A variable named '{:s}' "
                                                "exists in '{:s}' and overwriting "
                                                "was not requested.".format(
                                                    output_name,
                                                    netcdf_path ) )
        except Exception as e:
            print( "Cannot process '{:s}' ({:s}).".format(
                netcdf_path,
                str( e ) ),
                   file=sys.stderr )

            return 1

    # create the transformer object.  our transformer specifications are
    # validated here.
    try:
        netcdf_transformer = iwp.transforms.IWPnetCDFTransformer( arguments.reference_name,
                                                                  arguments.output_names,
                                                                  arguments.transform_specs,
                                                                  verbose_flag=options.verbose_flag )
    except Exception as e:
        print( "Could not create an netCDF4 transformer object ({:s}).".format(
            str( e ) ),
               file=sys.stderr )

    # launch our processing pool in separate processes.
    if options.verbose_flag:
        if options.number_workers == 1:
            print( "Processing transforms serially." )
        else:
            print( "Processing transforms with {:d} workers.".format(
                options.number_workers ) )

    try:
        mp_context = multiprocessing.get_context( method="spawn" )
        with concurrent.futures.ProcessPoolExecutor( max_workers=options.number_workers,
                                                     mp_context=mp_context ) as executor:

            # distribute the paths across the pool.  this is synchronous since
            # we don't need to overlap any work in the master process.
            work_statuses = list( executor.map( netcdf_transformer,
                                                arguments.netcdf_paths ) )
    except Exception as e:
        print( "An error was encountered while executing the transforms ({:s}).".format(
            str( e ) ),
               file=sys.stderr )

        return 1

    # convey any messages for processes that weren't successful.
    failed_statuses = list( filter( lambda x: x[0] != 0, work_statuses ) )
    if len( failed_statuses ) > 0:

        #
        # NOTE: mind the extra newline here.
        #
        print( "{:d} path{:s} encountered problems during processing:\n".format(
            len( failed_statuses ),
            "" if len( failed_statuses ) == 1 else "s" ) )

        for status_index, (status_code, status_message) in enumerate( work_statuses ):
            if status_code == 0:
                continue

            print( "  #{:d}. {:s} - {:s}".format(
                status_index + 1,
                arguments.netcdf_paths[status_index],
                work_statuses[status_index][1] ) )

    return (len( failed_statuses ) > 0)

if __name__ == "__main__":
    sys.exit( main( sys.argv ) )
